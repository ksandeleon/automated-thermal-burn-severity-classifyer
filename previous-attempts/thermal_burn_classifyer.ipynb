{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bbc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of old images: 9585\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to images\n",
    "image_dir = 'dataset/compiled-old/images'\n",
    "\n",
    "# Get list of image files (filtering out non-files and hidden files)\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and not f.startswith('.')]\n",
    "\n",
    "# Print total number of images\n",
    "print(f\"Total number of old images: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf326aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of new images: 967\n"
     ]
    }
   ],
   "source": [
    "# Set the path to images\n",
    "image_dir = 'dataset/compiled/images'\n",
    "\n",
    "# Get list of image files (filtering out non-files and hidden files)\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and not f.startswith('.')]\n",
    "\n",
    "# Print total number of images\n",
    "print(f\"Total number of new images: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4561ffc",
   "metadata": {},
   "source": [
    "almost 90% drop on dataset based on previous attempts. will limit data cleaning for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ad995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 24 duplicate pairs\n",
      "\n",
      "Encountered errors:\n",
      " - Missing file: dataset/compiled-new/images\\v3_img1414_jpg.rf.579eb39c366bb48651ad5a7216b9b843.jpg\n",
      " - Missing file: dataset/compiled-new/labels\\v3_img1414_jpg.rf.579eb39c366bb48651ad5a7216b9b843.txt\n",
      " - Missing file: dataset/compiled-new/images\\v3_img209_jpg.rf.69418453eb5bc826a35acf8bd440ecdc.jpg\n",
      " - Missing file: dataset/compiled-new/labels\\v3_img209_jpg.rf.69418453eb5bc826a35acf8bd440ecdc.txt\n",
      " - Missing file: dataset/compiled-new/images\\v3_img209_jpg.rf.92b0690ceeb149945e5fd1cee86eafd5.jpg\n",
      " - ...and 167 more errors\n",
      "Remaining images: 9561\n",
      "Remaining labels: 9561\n"
     ]
    }
   ],
   "source": [
    "import imagehash\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_dir = 'dataset/compiled-new/images'\n",
    "label_dir = 'dataset/compiled-new/labels'\n",
    "\n",
    "def find_duplicates(image_dir, label_dir, hash_size=8):\n",
    "    \"\"\"Returns a list of duplicate image pairs with their annotations.\"\"\"\n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "\n",
    "    # Supported image extensions\n",
    "    image_exts = ('.jpg', '.jpeg', '.png', '.webp')\n",
    "\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if not img_file.lower().endswith(image_exts):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "        try:\n",
    "            # Calculate perceptual hash\n",
    "            with Image.open(img_path) as img:\n",
    "                img_hash = imagehash.average_hash(img, hash_size=hash_size)\n",
    "\n",
    "            # Check for duplicates\n",
    "            if img_hash in hashes:\n",
    "                duplicates.append((img_path, label_path, hashes[img_hash]))\n",
    "            else:\n",
    "                hashes[img_hash] = (img_path, label_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "def handle_duplicates(duplicates, strategy='keep_best_annotation'):\n",
    "    \"\"\"\n",
    "    More robust duplicate handler that:\n",
    "    1. Checks if files exist before processing\n",
    "    2. Provides detailed error reporting\n",
    "    3. Handles edge cases\n",
    "    \"\"\"\n",
    "    deleted_files = []\n",
    "    errors = []\n",
    "\n",
    "    for img1, label1, (img2, label2) in duplicates:\n",
    "        try:\n",
    "            # Verify all files exist\n",
    "            existing_files = []\n",
    "            for path in [img1, label1, img2, label2]:\n",
    "                if os.path.exists(path):\n",
    "                    existing_files.append(path)\n",
    "                else:\n",
    "                    errors.append(f\"Missing file: {path}\")\n",
    "\n",
    "            # Skip if critical files are missing\n",
    "            if len(existing_files) < 4:\n",
    "                continue\n",
    "\n",
    "            # Determine which pair to keep\n",
    "            if strategy == 'keep_first':\n",
    "                to_delete = (img2, label2)\n",
    "            elif strategy == 'keep_best_annotation':\n",
    "                # Safely compare annotations\n",
    "                count1, count2 = 0, 0\n",
    "                try:\n",
    "                    with open(label1, 'r') as f1:\n",
    "                        count1 = len([line for line in f1 if line.strip()])\n",
    "                    with open(label2, 'r') as f2:\n",
    "                        count2 = len([line for line in f2 if line.strip()])\n",
    "                except Exception as e:\n",
    "                    errors.append(f\"Error reading annotations: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                to_delete = (img2, label2) if count1 >= count2 else (img1, label1)\n",
    "            elif strategy == 'keep_higher_res':\n",
    "                try:\n",
    "                    size1 = os.path.getsize(img1)\n",
    "                    size2 = os.path.getsize(img2)\n",
    "                except Exception as e:\n",
    "                    errors.append(f\"Error getting file sizes: {str(e)}\")\n",
    "                    continue\n",
    "                to_delete = (img2, label2) if size1 >= size2 else (img1, label1)\n",
    "\n",
    "            # Delete files (with existence check)\n",
    "            for file in to_delete:\n",
    "                if os.path.exists(file):\n",
    "                    os.remove(file)\n",
    "                    deleted_files.append(file)\n",
    "                else:\n",
    "                    errors.append(f\"Tried to delete non-existent file: {file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error processing pair {img1} vs {img2}: {str(e)}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Deleted {len(deleted_files)//2} duplicate pairs\")\n",
    "    if errors:\n",
    "        print(\"\\nEncountered errors:\")\n",
    "        for error in errors[:5]:  # Show first 5 errors\n",
    "            print(f\" - {error}\")\n",
    "        if len(errors) > 5:\n",
    "            print(f\" - ...and {len(errors)-5} more errors\")\n",
    "\n",
    "    return deleted_files, errors\n",
    "\n",
    "# Step 1: Find all duplicates\n",
    "duplicates = find_duplicates(image_dir, label_dir)\n",
    "\n",
    "# Step 3: Handle duplicates (choose strategy)\n",
    "handle_duplicates(duplicates, strategy='keep_best_annotation')\n",
    "\n",
    "# Step 4: Verify remaining files\n",
    "print(f\"Remaining images: {len(os.listdir(image_dir))}\")\n",
    "print(f\"Remaining labels: {len(os.listdir(label_dir))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b811bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Validation Report ===\n",
      "\n",
      "[!] 0 images missing labels:\n",
      "\n",
      "[!] 0 labels missing images:\n",
      "Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def validate_pairs(image_dir, label_dir):\n",
    "    \"\"\"Comprehensive validation of image-label pairs with detailed reporting\"\"\"\n",
    "    # Get all files (case-insensitive)\n",
    "    image_files = defaultdict(list)\n",
    "    label_files = set()\n",
    "\n",
    "    # Supported image extensions\n",
    "    img_exts = ('.jpg', '.jpeg', '.png', '.webp')\n",
    "\n",
    "    # Scan image directory\n",
    "    for f in os.listdir(image_dir):\n",
    "        base, ext = os.path.splitext(f)\n",
    "        if ext.lower() in img_exts:\n",
    "            image_files[base.lower()].append(f)  # Store with original case\n",
    "\n",
    "    # Scan label directory\n",
    "    for f in os.listdir(label_dir):\n",
    "        if f.lower().endswith('.txt'):\n",
    "            label_files.add(os.path.splitext(f)[0].lower())\n",
    "\n",
    "    # Find mismatches\n",
    "    missing_labels = set(image_files.keys()) - label_files\n",
    "    missing_images = label_files - set(image_files.keys())\n",
    "\n",
    "    # Generate detailed report\n",
    "    report = {\n",
    "        'images_without_labels': [],\n",
    "        'labels_without_images': [],\n",
    "        'multiple_images': []\n",
    "    }\n",
    "\n",
    "    # Check for images without labels\n",
    "    for base in missing_labels:\n",
    "        for img_file in image_files[base]:\n",
    "            report['images_without_labels'].append({\n",
    "                'image': img_file,\n",
    "                'possible_label': f\"{os.path.splitext(img_file)[0]}.txt\"\n",
    "            })\n",
    "\n",
    "    # Check for labels without images\n",
    "    for base in missing_images:\n",
    "        report['labels_without_images'].append({\n",
    "            'label': f\"{base}.txt\",\n",
    "            'possible_images': [f\"{base}{ext}\" for ext in img_exts]\n",
    "        })\n",
    "\n",
    "    # Check for multiple image extensions\n",
    "    for base, files in image_files.items():\n",
    "        if len(files) > 1:\n",
    "            report['multiple_images'].append({\n",
    "                'base': base,\n",
    "                'files': files,\n",
    "                'label_exists': base in label_files\n",
    "            })\n",
    "\n",
    "    return report\n",
    "\n",
    "def print_report(report):\n",
    "    \"\"\"Print a human-readable validation report\"\"\"\n",
    "    print(\"\\n=== Dataset Validation Report ===\")\n",
    "\n",
    "    # Section 1: Images without labels\n",
    "    print(f\"\\n[!] {len(report['images_without_labels'])} images missing labels:\")\n",
    "    for item in report['images_without_labels'][:5]:  # Show first 5 examples\n",
    "        print(f\"  - Image: {item['image']}\")\n",
    "        print(f\"    Expected label: {item['possible_label']}\")\n",
    "    if len(report['images_without_labels']) > 5:\n",
    "        print(f\"    (...and {len(report['images_without_labels']) - 5} more)\")\n",
    "\n",
    "    # Section 2: Labels without images\n",
    "    print(f\"\\n[!] {len(report['labels_without_images'])} labels missing images:\")\n",
    "    for item in report['labels_without_images'][:5]:\n",
    "        print(f\"  - Label: {item['label']}\")\n",
    "        print(f\"    Expected image variants: {', '.join(item['possible_images'])}\")\n",
    "    if len(report['labels_without_images']) > 5:\n",
    "        print(f\"    (...and {len(report['labels_without_images']) - 5} more)\")\n",
    "\n",
    "    # Section 3: Multiple image extensions\n",
    "    if report['multiple_images']:\n",
    "        print(f\"\\n[!] {len(report['multiple_images'])} base names with multiple images:\")\n",
    "        for item in report['multiple_images'][:3]:\n",
    "            print(f\"  - Base: {item['base']}\")\n",
    "            print(f\"    Files: {', '.join(item['files'])}\")\n",
    "            print(f\"    Has label: {'Yes' if item['label_exists'] else 'No'}\")\n",
    "        if len(report['multiple_images']) > 3:\n",
    "            print(f\"    (...and {len(report['multiple_images']) - 3} more)\")\n",
    "\n",
    "\n",
    "report = validate_pairs(image_dir, label_dir)\n",
    "print_report(report)\n",
    "\n",
    "# Optional: Auto-clean with confirmation\n",
    "if input(\"\\nClean up missing files? (y/n): \").lower() == 'y':\n",
    "    # Clean images without labels\n",
    "    for item in report['images_without_labels']:\n",
    "        img_path = os.path.join(image_dir, item['image'])\n",
    "        os.remove(img_path)\n",
    "        print(f\"Removed {img_path}\")\n",
    "\n",
    "    # Clean labels without images\n",
    "    for item in report['labels_without_images']:\n",
    "        label_path = os.path.join(label_dir, item['label'])\n",
    "        if os.path.exists(label_path):\n",
    "            os.remove(label_path)\n",
    "            print(f\"Removed {label_path}\")\n",
    "\n",
    "    print(\"Cleanup complete!\")\n",
    "else:\n",
    "    print(\"No files were deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a76e8a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 9561\n"
     ]
    }
   ],
   "source": [
    "# Set the path to images\n",
    "image_dir = 'dataset/compiled-new/images'\n",
    "\n",
    "# Get list of image files (filtering out non-files and hidden files)\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and not f.startswith('.')]\n",
    "\n",
    "# Print total number of images\n",
    "print(f\"Total number of images: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95dbec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "First Degree Burn (Class 0): 5047 images\n",
      "Third Degree Burn (Class 2): 2126 images\n",
      "Second Degree Burn (Class 1): 5657 images\n",
      "\n",
      "Total labeled images: 12830\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Directories\n",
    "label_dir = 'dataset/compiled-new/labels'\n",
    "\n",
    "# Initialize class counter\n",
    "class_counts = Counter()\n",
    "\n",
    "# Go through each label file\n",
    "for label_file in os.listdir(label_dir):\n",
    "    if label_file.endswith('.txt'):\n",
    "        with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                class_id = line.strip().split()[0]  # get the class ID\n",
    "                class_counts[class_id] += 1\n",
    "\n",
    "# Map class IDs to degree names\n",
    "class_map = {'0': 'First Degree Burn', '1': 'Second Degree Burn', '2': 'Third Degree Burn'}\n",
    "\n",
    "# Print distribution\n",
    "print(\"Class Distribution:\")\n",
    "for class_id, count in class_counts.items():\n",
    "    print(f\"{class_map.get(class_id, 'Unknown')} (Class {class_id}): {count} images\")\n",
    "\n",
    "# Optional: show total images with labels\n",
    "total_labeled = sum(class_counts.values())\n",
    "print(f\"\\nTotal labeled images: {total_labeled}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "457febd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (one label per image):\n",
      "First Degree Burn (Class 0): 3925 images\n",
      "Third Degree Burn (Class 2): 1564 images\n",
      "Second Degree Burn (Class 1): 4072 images\n",
      "\n",
      "Total labeled images: 9561\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "label_dir = 'dataset/compiled-new/labels'\n",
    "class_counts = Counter()\n",
    "\n",
    "for label_file in os.listdir(label_dir):\n",
    "    if label_file.endswith('.txt'):\n",
    "        with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "            first_line = f.readline()\n",
    "            if first_line:\n",
    "                class_id = first_line.strip().split()[0]\n",
    "                class_counts[class_id] += 1\n",
    "\n",
    "class_map = {'0': 'First Degree Burn', '1': 'Second Degree Burn', '2': 'Third Degree Burn'}\n",
    "\n",
    "print(\"Class Distribution (one label per image):\")\n",
    "for class_id, count in class_counts.items():\n",
    "    print(f\"{class_map.get(class_id, 'Unknown')} (Class {class_id}): {count} images\")\n",
    "\n",
    "total_labeled = sum(class_counts.values())\n",
    "print(f\"\\nTotal labeled images: {total_labeled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f960c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label files with no matching image: 0\n"
     ]
    }
   ],
   "source": [
    "# Check how many label files have no corresponding image\n",
    "label_dir = 'dataset/compiled-new/labels'\n",
    "image_stems = set(os.path.splitext(f)[0] for f in image_files)\n",
    "label_stems = set(os.path.splitext(f)[0] for f in os.listdir(label_dir) if f.endswith('.txt'))\n",
    "\n",
    "no_image = label_stems - image_stems\n",
    "print(f\"Label files with no matching image: {len(no_image)}\")\n",
    "if no_image:\n",
    "    print(\"Example:\", list(no_image)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "703cad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "image_dir = \"dataset/compiled-new/images\"\n",
    "label_dir = \"dataset/compiled-new/labels\"\n",
    "output_base = \"dataset/dataset_classified_split\"\n",
    "\n",
    "# Gather (image, class_id) pairs\n",
    "samples = []\n",
    "for img_name in os.listdir(image_dir):\n",
    "    if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        continue\n",
    "    label_name = os.path.splitext(img_name)[0] + \".txt\"\n",
    "    label_path = os.path.join(label_dir, label_name)\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "    if not os.path.exists(label_path):\n",
    "        continue\n",
    "    with open(label_path, \"r\") as f:\n",
    "        first_line = f.readline().strip()\n",
    "        if not first_line:\n",
    "            continue\n",
    "        class_id = first_line.split()[0]\n",
    "    samples.append((img_path, class_id))\n",
    "\n",
    "# Stratified split\n",
    "img_paths, class_ids = zip(*samples)\n",
    "train_imgs, test_imgs, train_labels, test_labels = train_test_split(\n",
    "    img_paths, class_ids, test_size=0.2, stratify=class_ids, random_state=42\n",
    ")\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
    "    train_imgs, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "splits = [\n",
    "    (\"train\", train_imgs, train_labels),\n",
    "    (\"val\", val_imgs, val_labels),\n",
    "    (\"test\", test_imgs, test_labels),\n",
    "]\n",
    "\n",
    "# Copy images into split/class folders\n",
    "for split_name, imgs, labels in splits:\n",
    "    for img_path, class_id in zip(imgs, labels):\n",
    "        split_dir = os.path.join(output_base, split_name, class_id)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        shutil.copy(img_path, os.path.join(split_dir, os.path.basename(img_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3364bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Split Distribution:\n",
      "Train: {'0': 2512, '1': 2605, '2': 1001}\n",
      "Val: {'0': 628, '2': 250, '1': 652}\n",
      "Test: {'2': 313, '0': 785, '1': 815}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Final Split Distribution:\")\n",
    "for name, labels in [(\"Train\", train_labels), (\"Val\", val_labels), (\"Test\", test_labels)]:\n",
    "    counts = Counter(labels)\n",
    "    print(f\"{name}: {dict(counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbd7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#1.1 Data augmentation pipeline\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "    keras.layers.RandomBrightness(0.1)\n",
    "])\n",
    "\n",
    "# Normalization layer\n",
    "normalization_layer = keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c60743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6118 files belonging to 3 classes.\n",
      "Found 1530 files belonging to 3 classes.\n",
      "Found 1530 files belonging to 3 classes.\n",
      "Found 1913 files belonging to 3 classes.\n",
      "Found 1913 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#1.2 load data\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    \"dataset/dataset_classified_split/train\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    \"dataset/dataset_classified_split/val\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    \"dataset/dataset_classified_split/test\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Apply augmentation and normalization\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(normalization_layer(x)), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f19624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "\n",
    "# #1.3 build model - pretrained resnet50\n",
    "# def build_resnet_classifier(input_shape=IMG_SIZE + (3,), num_classes=3):\n",
    "#     base_model = keras.applications.ResNet50(\n",
    "#         include_top=False,\n",
    "#         weights=\"imagenet\",\n",
    "#         input_shape=input_shape,\n",
    "#         pooling=\"avg\"\n",
    "#     )\n",
    "#     base_model.trainable = False  # Fine-tune later if needed\n",
    "\n",
    "#     inputs = keras.Input(shape=input_shape)\n",
    "#     x = base_model(inputs, training=False)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     x = layers.Dense(128, activation=\"relu\")(x)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "#     model = keras.Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "# resnet_model = build_resnet_classifier()\n",
    "# resnet_model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(),\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "# resnet_model.summary()\n",
    "\n",
    "#result = 45% accuracy after 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling2D, Reshape, Dense, Input, Dropout, Flatten\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# inputs = Input(shape=(224, 224, 3))\n",
    "# # CNN backbone\n",
    "# resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# resnet.trainable = False\n",
    "# x = resnet(inputs)\n",
    "\n",
    "# # Convert CNN features to 1D\n",
    "# x = GlobalAveragePooling2D()(x)         # (batch, 2048)\n",
    "# x = Reshape((1, -1))(x)                 # (batch, 1, 2048)\n",
    "\n",
    "# # Transformer block (self-attention)\n",
    "# x = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "# x = LayerNormalization()(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# model = Model(inputs=inputs, outputs=outputs)\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(),\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "# model.summary()\n",
    "\n",
    "#result 42% accuracy after 10 epochs\n",
    "#was only able to classify class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6c4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 10\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=EPOCHS\n",
    "# )\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(test_ds)\n",
    "# print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2b11f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.8119745222929936), 1: np.float64(0.7826620825147348), 2: np.float64(2.03772378516624)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ resnet50            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ resnet50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,968</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ resnet50            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │ \u001b[38;5;34m23,587,712\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ resnet50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2048\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2048\u001b[0m)   │  \u001b[38;5;34m2,099,968\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2048\u001b[0m)   │      \u001b[38;5;34m4,096\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m262,272\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │        \u001b[38;5;34m387\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,954,435</span> (99.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,954,435\u001b[0m (99.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,832,387</span> (26.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,832,387\u001b[0m (26.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,122,048</span> (72.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m19,122,048\u001b[0m (72.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 2s/step - accuracy: 0.3601 - loss: 1.1377 - val_accuracy: 0.1634 - val_loss: 1.2019 - learning_rate: 1.0000e-05\n",
      "Epoch 2/30\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 1s/step - accuracy: 0.3656 - loss: 1.0841 - val_accuracy: 0.4261 - val_loss: 1.0783 - learning_rate: 1.0000e-05\n",
      "Epoch 3/30\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 2s/step - accuracy: 0.3624 - loss: 1.0853 - val_accuracy: 0.2072 - val_loss: 1.1337 - learning_rate: 1.0000e-05\n",
      "Epoch 4/30\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3588 - loss: 1.0709\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 2s/step - accuracy: 0.3604 - loss: 1.0778 - val_accuracy: 0.3686 - val_loss: 1.1082 - learning_rate: 1.0000e-05\n",
      "Epoch 5/30\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 2s/step - accuracy: 0.3995 - loss: 1.0624 - val_accuracy: 0.3752 - val_loss: 1.0876 - learning_rate: 5.0000e-06\n",
      "Epoch 6/30\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3973 - loss: 1.0483\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 2s/step - accuracy: 0.3856 - loss: 1.0628 - val_accuracy: 0.3948 - val_loss: 1.0887 - learning_rate: 5.0000e-06\n",
      "Epoch 7/30\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 2s/step - accuracy: 0.3993 - loss: 1.0589 - val_accuracy: 0.4092 - val_loss: 1.0883 - learning_rate: 2.5000e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling2D, Reshape, Dense, Input, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "class_counts = [3925, 4072, 1564]  # [class 0, class 1, class 2]\n",
    "class_labels = np.array([0]*class_counts[0] + [1]*class_counts[1] + [2]*class_counts[2])\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(class_labels), y=class_labels)\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "# CNN backbone\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet.trainable = True\n",
    "for layer in resnet.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "x = resnet(inputs)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Reshape((1, -1))(x)\n",
    "x = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "x = LayerNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Callbacks for better training\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
